{"cells":[{"cell_type":"markdown","metadata":{"id":"P49_3OsLgHq-"},"source":["# 1. Setup"]},{"cell_type":"markdown","metadata":{"id":"SQ8S9q9qgHq_"},"source":["## Set environment and device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4994,"status":"ok","timestamp":1731220879803,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"},"user_tz":-480},"id":"EovtURCvgHrA","outputId":"86cb5dfd-9afe-40ee-d60a-e5933bb2e1ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/AmazonReviews\n"]}],"source":["# Mount to Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Define project folder\n","FOLDERNAME = 'Colab\\ Notebooks/AmazonReviews'\n","\n","%cd drive/MyDrive/$FOLDERNAME"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3512,"status":"ok","timestamp":1731220883313,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"},"user_tz":-480},"id":"KRVq5bzXgHrA","outputId":"8dc56faf-2837-413f-f4ab-14d0d87f761b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}],"source":["# Define device\n","import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Device: {device}')"]},{"cell_type":"markdown","metadata":{"id":"LvSmvpZAgHrB"},"source":["## Set random seed"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1731220883313,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"},"user_tz":-480},"id":"xxwZWc-_gHrB","outputId":"3466cf57-8e44-45d9-bb6f-6227e67a39cc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7914b41510f0>"]},"metadata":{},"execution_count":3}],"source":["torch.manual_seed(77)  # For reproducibility"]},{"cell_type":"markdown","metadata":{"id":"nmcctl9hgHrB"},"source":["# 2. Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"XpTdG4HZeNHH"},"source":["## Download dataset"]},{"cell_type":"markdown","metadata":{"id":"T8rnRxodNrS4"},"source":["### Set up Kaggle API key"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1731217168372,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"},"user_tz":-480},"id":"ot_xxJd4NrnB","outputId":"0cd8c598-9d67-46dd-f1f5-b09ddc800130"},"outputs":[{"output_type":"stream","name":"stdout","text":["Kaggle API setup completed successfully.\n"]}],"source":["import os\n","import json\n","\n","# Path to the Kaggle API key JSON file\n","KAGGLE_API_KEY_PATH = '/content/drive/MyDrive/Colab Notebooks/credentials/kaggle/kaggle.json'\n","\n","\n","# Kaggle API checks for credentials in this order:\n","# - Environment variables (KAGGLE_USERNAME and KAGGLE_KEY)\n","# - Kaggle config file (Kaggle API expects it to be found at ~/.kaggle/kaggle.json)\n","def setup_kaggle():\n","    # This function reads a json file and store the API info in the environment variables.\n","    if os.path.exists(KAGGLE_API_KEY_PATH):\n","        try:\n","            with open(KAGGLE_API_KEY_PATH, 'r') as f:\n","                kaggle_api_key = json.load(f)\n","\n","            os.environ['KAGGLE_USERNAME'] = kaggle_api_key['username']\n","            os.environ['KAGGLE_KEY'] = kaggle_api_key['key']\n","\n","            print(\"Kaggle API setup completed successfully.\")\n","\n","        except Exception as e:\n","            print(f\"Error setting up Kaggle credentials: {e}\")\n","\n","    else:\n","        print(f\"Kaggle API key file not found at {KAGGLE_API_KEY_PATH}\")\n","\n","setup_kaggle()"]},{"cell_type":"markdown","metadata":{"id":"xkaIz196hNlY"},"source":["### Download Kaggle dataset using Kaggle API\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":558,"status":"ok","timestamp":1731217168928,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"},"user_tz":-480},"id":"FWDAvjZRhy9s","outputId":"2298e0ee-71c1-4a25-9d5b-902ad3b478b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews\n","Dataset downloaded successfully to /content/drive/MyDrive/Colab Notebooks/raw datasets/kaggle/amazon-reviews\n","\n","Downloaded files:\n","- amazon-reviews.zip\n","- test.csv\n","- train.csv\n","- amazon_review_polarity_csv.tgz\n"]}],"source":["from pathlib import Path\n","import kaggle\n","\n","# Define the base directory for datasets\n","BASE_DATASET_DIR = '/content/drive/MyDrive/Colab Notebooks/raw datasets/kaggle'\n","\n","def download_kaggle_dataset(dataset_name, dataset_folder=None):\n","    \"\"\"\n","    Download and unzip a Kaggle dataset\n","\n","    Parameters:\n","    dataset_name (str): Name of dataset (e.g., 'username/dataset-name')\n","    dataset_folder (str): Optional subfolder name. If None, uses last part of dataset_name\n","    \"\"\"\n","    try:\n","        # If no specific folder name is provided, use the dataset name\n","        if dataset_folder is None:\n","            dataset_folder = dataset_name.split('/')[-1]\n","\n","        # Create the full save directory path\n","        save_dir = os.path.join(BASE_DATASET_DIR, dataset_folder)\n","\n","        # Create save directory if it doesn't exist\n","        save_path = Path(save_dir)\n","        save_path.mkdir(parents=True, exist_ok=True)\n","\n","        # Download the dataset\n","        kaggle.api.dataset_download_files(\n","            dataset_name,\n","            path=save_dir,\n","            unzip=True  # Set to False if you want to keep the zip file\n","        )\n","\n","        print(f\"Dataset downloaded successfully to {save_dir}\")\n","\n","        # List remaining files\n","        print(\"\\nDownloaded files:\")\n","        for file in save_path.glob('*'):\n","            print(f\"- {file.name}\")\n","\n","    except Exception as e:\n","        print(f\"Error downloading dataset: {e}\")\n","\n","download_kaggle_dataset('kritanjalijain/amazon-reviews')"]},{"cell_type":"markdown","metadata":{"id":"qLf4TL23eNHI"},"source":["## Explore dataset to find out what preprocessing steps are needed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5kWGjbiJ28i"},"outputs":[],"source":["import pandas as pd\n","\n","train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/raw datasets/kaggle/amazon-reviews/train.csv', header=None, names=['label', 'title', 'review'])\n","\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QWJQteR4K7Tm"},"outputs":[],"source":["# Check missing value in title and review columns\n","print('title NaN: ', len(train_df[train_df.title.isnull()]))\n","print('review NaN: ', len(train_df[train_df.review.isnull()]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llAzvYcvMhek"},"outputs":[],"source":["# Combine review title and review columns to a new column 'full_review'\n","# Replace any NaN values in the title or review with an empty string\n","train_df['full_review'] = train_df.title.fillna('') + \" \" + train_df.review.fillna('')\n","\n","# Check if full_review is combined correctly, not effected by NaN title or review\n","train_df[train_df.title.isnull() | train_df.review.isnull()].head()"]},{"cell_type":"markdown","metadata":{"id":"yWBJd8gyeNHJ"},"source":["### Label distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aU8HofpIeNHJ"},"outputs":[],"source":["# Check label distribution (check if the dataset is balanced\n","pd.Series(train_df.label).value_counts()  # pandas.value_counts is deprecated"]},{"cell_type":"markdown","metadata":{"id":"qgkZVCpIeNHJ"},"source":["### Length distribution --> sequence length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RStLkP7FeNHJ"},"outputs":[],"source":["# Check dataset length distribution (length of string in 'full_review' column)\n","# This can help to determine the sequence length for the model\n","chunk_size = 1000  # Process in chunks to avoid memory issues\n","word_lengths = []\n","\n","for i in range(0, len(train_df), chunk_size):\n","    chunk = train_df['full_review'].iloc[i:i+chunk_size]\n","    # Process each review individually\n","    for review in chunk:\n","        # Quick and rough word count using split()\n","        word_lengths.append(len(str(review).split()))\n","\n","# Convert list to pandas Series for easy statistics\n","word_lengths = pd.Series(word_lengths)\n","\n","\n","# Print basic statistics\n","print(\"Word count statistics:\")\n","print(word_lengths.describe())\n","\n","\"\"\"\n","Word count statistics:\n","count    3.600000e+06\n","mean     7.848268e+01\n","std      4.283283e+01\n","min      2.000000e+00\n","25%      4.200000e+01\n","50%      7.000000e+01\n","75%      1.080000e+02\n","max      2.570000e+02\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"Em9YiODFeNHJ"},"source":["### Word frequency distribution --> corpus size, stop words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXpzJgyUeNHK"},"outputs":[],"source":["# Check word frequency distribution\n","# How many words show up in the dataset and how many times each word shows up\n","# This can help to determine the vocabulary (corpus) size for the model\n","\n","# The Oxford English Dictionary: ~600k words, incl. both current and obsolete words\n","# Merriam-Websterâ€™s Dictionary: ~470k words\n","# Studies suggest that most people use around 20k-35k words in their active vocabulary\n","from collections import Counter\n","\n","chunk_size = 1000  # Process in chunks to avoid memory issues\n","word_freq = Counter()\n","\n","for i in range(0, len(train_df), chunk_size):\n","    chunk = train_df['full_review'].iloc[i:i+chunk_size]\n","    for review in chunk:\n","        words = str(review).lower().split()\n","        word_freq.update(words)\n","\n","# Print basic statistics\n","print(f\"Total unique words: {len(word_freq)}\")\n","print(\"\\nMost common words:\")\n","for word, count in word_freq.most_common(20):\n","    print(f\"{word}: {count}\")\n","\n","# Look at frequency distribution\n","print(\"\\nWord frequency distribution:\")\n","freq_dist = Counter([count for count in word_freq.values()])\n","print(\"Words appearing:\")\n","for freq in sorted(freq_dist.keys())[:5]:\n","    print(f\"{freq} times: {freq_dist[freq]} words\")\n","\n","# Vocabulary filtering\n","MIN_FREQ = 8  # Only keep words appearing 8+ times\n","filtered_vocab = {word: count for word, count in word_freq.items()\n","                 if count >= MIN_FREQ}\n","print(f\"\\nFiltered vocabulary size: {len(filtered_vocab)}\")\n","\"\"\"Filtered vocabulary size: 342294  # MIN_FREQ = 8\"\"\""]},{"cell_type":"markdown","metadata":{"id":"d3xcXtHzeNHK"},"source":["### Check for HTML tags, URLs and special characters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyZSHb3DeNHK"},"outputs":[],"source":["# Look for HTML tags and URLs\n","# Quick check for HTML tags and URLs in a random sample\n","import re  # Regular expression\n","\n","train_df_sample = train_df.sample(n=6000, random_state=33).astype(str)\n","\n","html_count = train_df_sample['full_review'].str.contains(r'<[^>]+>').sum()\n","url_count = train_df_sample['full_review'].str.contains(r'http[s]?://').sum()\n","\n","print(f\"Reviews with HTML tags: {html_count}\")\n","print(f\"Reviews with URLs: {url_count}\")\n","\n","# Find special symbols\n","# Get and count special characters\n","special_chars = ''\n","for review in train_df_sample['full_review']:\n","    chars = re.findall(r'[^a-zA-Z0-9\\s]', str(review))\n","    special_chars += ''.join(chars)\n","\n","print(\"\\nMost common special characters:\")\n","for char, count in Counter(special_chars).most_common(15):\n","    print(f\"'{char}': {count}\")\n","\n","# Check for unusual punctuation\n","punct_patterns = re.findall(r'[^\\w\\s]{2,}', ' '.join(train_df_sample['full_review']))\n","print(\"\\nUnusual punctuation patterns:\")\n","punct_counter = Counter(punct_patterns)\n","for pattern, count in punct_counter.most_common(10):\n","    print(f\"'{pattern}': {count}\")\n","\n","# Check for repeated punctuation\n","repeated_punct = re.findall(r'([!?\\.]{2,})', ' '.join(train_df_sample['full_review']))\n","print(\"\\nRepeated punctuation:\")\n","repeat_counter = Counter(repeated_punct)\n","for pattern, count in repeat_counter.most_common(10):\n","        print(f\"'{pattern}': {count}\")\n"]},{"cell_type":"markdown","metadata":{"id":"JA0wWmzbeNHK"},"source":["## Preprocess dataset before loading into Dataset class\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGiBbd4peNHK"},"outputs":[],"source":["\n","\n","# # Define patterns\n","# url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')\n","# repeated_char_pattern = re.compile(r'(.)\\1{3,}')  # e.g., 'loooove' -> 'love'\n","# repeated_word_pattern = re.compile(r'\\b(\\w+)( \\1\\b)+')  # e.g., 'very very' -> 'very'\n","\n","# # Process a string\n","# def simple_preprocess(text):\n","#     # Convert to lowercase\n","#     text = str(text).lower()\n","\n","#     # Remove URLs\n","#     text = url_pattern.sub('', text)\n","#     # Remove repeated characters\n","#     text = repeated_char_pattern.sub(r'\\1', text)\n","\n","#     text = repeated_word_pattern.sub(r'\\1', text)\n","\n","#     # Basic cleaning (no SpaCy)\n","#     text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n","#     text = re.sub(r'\\d+', '', text)      # Remove numbers\n","#     return ' '.join(text.split())\n","\n","# # Test\n","# # text = \"I loooove this product!!! Check it out at http://example.com\"\n","# # clean_text = simple_preprocess(text)\n","# # print(f\"Original: {text}\")\n","# # print(f\"Cleaned: {clean_text}\")\n","\n","# # Process multiple texts (DataFrame)\n","# from joblib import Parallel, delayed\n","# from tqdm import tqdm\n","\n","# def process_multiple_texts_parallel(texts, n_jobs=-1):\n","#     \"\"\"\n","#     Process texts in parallel using joblib\n","#     n_jobs=-1 means use all available cores\n","#     \"\"\"\n","#     cleaned_texts = Parallel(n_jobs=n_jobs, backend=\"multiprocessing\")(\n","#         delayed(simple_preprocess)(text) for text in tqdm(texts, desc=\"Preprocessing texts\",\n","#             miniters=len(texts)//100)\n","#     )\n","#     return cleaned_texts\n","\n","# # Process the reviews\n","# cleaned_reviews = process_multiple_texts_parallel(train_df['full_review'])\n","# train_df['cleaned_review'] = cleaned_reviews\n","\n","# # Check the results\n","# print(\"\\nExample of original vs cleaned review:\")\n","# print(\"Original:\", train_df['full_review'].iloc[0])\n","# print(\"Cleaned:\", train_df['cleaned_review'].iloc[0])\n","\n","# # Check a few more examples\n","# print(\"\\nMore examples:\")\n","# for i in range(5):\n","#     print(f\"\\nExample {i+1}:\")\n","#     print(\"Original:\", train_df['full_review'].iloc[i])\n","#     print(\"Cleaned:\", train_df['cleaned_review'].iloc[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_NYCyvcpeNHK","executionInfo":{"status":"ok","timestamp":1731220923838,"user_tz":-480,"elapsed":31081,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"}},"outputId":"77646076-424c-4b91-ef28-5554412e6fed"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-c558698cf0f5>:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  df_sample = df.groupby(stratify_col).apply(\n"]},{"output_type":"stream","name":"stdout","text":["Train labels unique values: [0 1]\n","Train labels value counts:\n"," label\n","0    5000\n","1    5000\n","Name: count, dtype: int64\n","\n","Validation labels unique values: [0 1]\n","Validation labels value counts:\n"," label\n","0    1000\n","1    1000\n","Name: count, dtype: int64\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-c558698cf0f5>:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  df_sample = df.groupby(stratify_col).apply(\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","# Load the data\n","train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/raw datasets/kaggle/amazon-reviews/train.csv',\n","                       header=None, names=['label', 'title', 'review'])\n","val_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/raw datasets/kaggle/amazon-reviews/test.csv',\n","                     header=None, names=['label', 'title', 'review'])\n","\n","# Set sample sizes\n","TRAIN_SIZE = 10_000\n","VAL_SIZE = 2_000\n","\n","# Sample with stratification\n","def stratified_sample(df, n, stratify_col='label'):\n","    df_sample = df.groupby(stratify_col).apply(\n","        lambda x: x.sample(n=n//len(df[stratify_col].unique()))\n","    ).reset_index(drop=True)\n","    return df_sample\n","\n","# Take balanced samples\n","train_df = stratified_sample(train_df, TRAIN_SIZE)\n","val_df = stratified_sample(val_df, VAL_SIZE)\n","\n","# Convert labels from 1/2 to 0/1\n","train_df['label'] = train_df['label'].map({1: 0, 2: 1})\n","val_df['label'] = val_df['label'].map({1: 0, 2: 1})\n","# Verify the conversion\n","print(\"Train labels unique values:\", train_df['label'].unique())\n","print(\"Train labels value counts:\\n\", train_df['label'].value_counts())\n","print(\"\\nValidation labels unique values:\", val_df['label'].unique())\n","print(\"Validation labels value counts:\\n\", val_df['label'].value_counts())\n","\n","# Combine review title and review columns to a new column 'full_review'\n","# Replace any NaN values in the title or review with an empty string\n","train_df['full_review'] = train_df.title.fillna('') + \" \" + train_df.review.fillna('')\n","val_df['full_review'] = val_df.title.fillna('') + \" \" + val_df.review.fillna('')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynUKEMKveNHK","executionInfo":{"status":"ok","timestamp":1731220974439,"user_tz":-480,"elapsed":524,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"}},"outputId":"718e5291-fa68-480f-c1eb-22139d8a7702"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing transform function:\n","\n","Example 1:\n","Original: doesn't work Don't buy , put a rib roast in and turned it on , heated up so we left ...came home to a cold pot with an uncooked and ruined $3o piece of meat....called customer service and were give the phone tag run around for 35 minutes and told there was nothing they could do about the cost of the meat which was Their fault...so if u buy one , good luck and don't bother with crappy c s dept !!\n","Transformed: doesnt work dont buy put turned left came home cold pot ruined piece customer service give phone tag run around minutes told nothing could cost meat u buy one good luck dont bother crappy c\n","\n","Example 2:\n","Original: We sent this back We have tied this swaddle, the \"swaddleme\" and the Amazing Miracle Blanket.We have twins (boy and girl) and not only did this swaddle not keep them secure, but we awoke to find the swaddle part wrapped around their necks - not good.Do yourself a favour and get the miracle blanket, it is the best of the 3.\n","Transformed: sent back tied amazing boy girl not not keep secure find part wrapped around not get blanket best\n","\n","Example 3:\n","Original: Italian cd defective Sounded like a scratch but was continuous throughout the producrion. Ithe interuption in the dialogue was very distracting. Would like replaced\n","Transformed: italian cd defective sounded like scratch continuous throughout dialogue distracting would like replaced\n"]}],"source":["import os\n","from collections import Counter\n","from nltk.corpus import stopwords\n","import nltk\n","import re\n","\n","# Define NLTK data path in Google Drive\n","nltk_data_path = '/content/drive/MyDrive/Colab Notebooks/nltk_data'\n","\n","# Create the directory if it doesn't exist\n","if not os.path.exists(nltk_data_path):\n","    os.makedirs(nltk_data_path)\n","\n","# Add the custom path to NLTK's data path\n","nltk.data.path.append(nltk_data_path)\n","\n","# Download stopwords if they don't exist\n","if not os.path.exists(os.path.join(nltk_data_path, 'corpora/stopwords')):\n","    nltk.download('stopwords', download_dir=nltk_data_path)\n","\n","\n","\n","NEGATION_WORDS = {'not', 'no', 'nor', 'never', 'none', 'neither', 'nowhere', 'nothing'}\n","STOP_WORDS = set(stopwords.words('english')) - NEGATION_WORDS\n","\n","# Create vocabulary from training data (do this once before training)\n","def create_vocabulary(texts, min_freq=8):\n","    word_freq = Counter()\n","    for text in texts:\n","        words = str(text).lower().split()\n","        word_freq.update(words)\n","\n","    # Filter vocabulary but keep negation words regardless of frequency\n","    return {word: count for word, count in word_freq.items()\n","            if (count >= min_freq and word not in STOP_WORDS) or word in NEGATION_WORDS}\n","\n","# Create vocabulary from training data\n","vocabulary = create_vocabulary(train_df['full_review'], min_freq=8)\n","\n","\n","# Define patterns\n","url_pattern = re.compile(r'http\\S+|www\\S+|https\\S+')\n","repeated_char_pattern = re.compile(r'(.)\\1{3,}')  # e.g., 'loooove' -> 'love'\n","repeated_word_pattern = re.compile(r'\\b(\\w+)( \\1\\b)+')  # e.g., 'very very' -> 'very'\n","\n","def transform(text):\n","    \"\"\"\n","    Default preprocessing transform - matches simple_preprocess function\n","    \"\"\"\n","    # Convert to lowercase\n","    text = str(text).lower()\n","\n","    # Remove URLs\n","    text = url_pattern.sub('', text)\n","    # Remove repeated characters\n","    text = repeated_char_pattern.sub(r'\\1', text)\n","    # Remove repeated words\n","    text = repeated_word_pattern.sub(r'\\1', text)\n","\n","    # Basic cleaning (no SpaCy)\n","    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n","    text = re.sub(r'\\d+', '', text)      # Remove numbers\n","\n","    # Split into words and filter\n","    words = text.split()\n","    # Keep only words that are in our vocabulary (this removes both stopwords and infrequent words)\n","    filtered_words = [word for word in words if word in vocabulary]\n","\n","    return ' '.join(filtered_words)\n","\n","# Test transform function\n","# Test the transform function on a few examples\n","print(\"Testing transform function:\")\n","for i in range(3):\n","    original = train_df['full_review'].iloc[i]\n","    transformed = transform(original)\n","    print(f\"\\nExample {i+1}:\")\n","    print(\"Original:\", original)\n","    print(\"Transformed:\", transformed)\n"]},{"cell_type":"markdown","metadata":{"id":"AFLNj8hSgHrC"},"source":["## Load datasets with transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9Av-X3kJ3T0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731221316340,"user_tz":-480,"elapsed":346,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"}},"outputId":"7a3599c4-aa50-4135-814d-9655d948f73d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset size: 10000\n","Sample input shape: torch.Size([300])\n","Sample label: 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["from torch.utils.data import Dataset\n","import torch\n","from transformers import BertTokenizer  # or another tokenizer of your choice\n","\n","class ReviewDataset(Dataset):\n","    def __init__(self, reviews, labels, max_length=300, transform=transform):\n","        self.reviews = reviews\n","        self.labels = labels\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.max_length = max_length\n","        # Transform\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.reviews)\n","\n","    def __getitem__(self, idx):\n","        review = str(self.reviews[idx])\n","        label = self.labels[idx]\n","\n","        # Apply transform to the review text\n","        review = self.transform(review)\n","\n","        # Tokenize and convert to tensor\n","        encoding = self.tokenizer(\n","            review,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'label': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# Create dataset\n","train_dataset = ReviewDataset(\n","    reviews=train_df['full_review'],\n","    labels=train_df['label'].values\n",")\n","\n","val_dataset = ReviewDataset(\n","    reviews=val_df['full_review'],\n","    labels=val_df['label'].values\n",")\n","\n","# Test the dataset\n","print(f\"Dataset size: {len(train_dataset)}\")\n","sample = train_dataset[0]\n","print(f\"Sample input shape: {sample['input_ids'].shape}\")\n","print(f\"Sample label: {sample['label']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7qzju73leNHK","executionInfo":{"status":"ok","timestamp":1731221316702,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"}},"outputId":"cd9ec47c-8ea0-46bd-e8d6-8eceba83239f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sample 1:\n","Original text: doesn't work Don't buy , put a rib roast in and turned it on , heated up so we left ...came home to a cold pot with an uncooked and ruined $3o piece of meat....called customer service and were give th...\n","Preprocessed text: doesnt work dont buy put turned left came home cold pot ruined piece customer service give phone tag run around minutes told nothing could cost meat u buy one good luck dont bother crappy c...\n","Label: 0\n","First 30 tokens: ['[CLS]', 'doesn', '##t', 'work', 'don', '##t', 'buy', 'put', 'turned', 'left', 'came', 'home', 'cold', 'pot', 'ruined', 'piece', 'customer', 'service', 'give', 'phone', 'tag', 'run', 'around', 'minutes', 'told', 'nothing', 'could', 'cost', 'meat', 'u']\n","Input tensor shape: torch.Size([300])\n","Attention mask shape: torch.Size([300])\n","----------------------------------------------------------------------------------------------------\n","\n","Sample 2:\n","Original text: We sent this back We have tied this swaddle, the \"swaddleme\" and the Amazing Miracle Blanket.We have twins (boy and girl) and not only did this swaddle not keep them secure, but we awoke to find the s...\n","Preprocessed text: sent back tied amazing boy girl not not keep secure find part wrapped around not get blanket best...\n","Label: 0\n","First 30 tokens: ['[CLS]', 'sent', 'back', 'tied', 'amazing', 'boy', 'girl', 'not', 'not', 'keep', 'secure', 'find', 'part', 'wrapped', 'around', 'not', 'get', 'blanket', 'best', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","Input tensor shape: torch.Size([300])\n","Attention mask shape: torch.Size([300])\n","----------------------------------------------------------------------------------------------------\n","\n","Sample 3:\n","Original text: Italian cd defective Sounded like a scratch but was continuous throughout the producrion. Ithe interuption in the dialogue was very distracting. Would like replaced...\n","Preprocessed text: italian cd defective sounded like scratch continuous throughout dialogue distracting would like replaced...\n","Label: 0\n","First 30 tokens: ['[CLS]', 'italian', 'cd', 'defective', 'sounded', 'like', 'scratch', 'continuous', 'throughout', 'dialogue', 'distracting', 'would', 'like', 'replaced', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","Input tensor shape: torch.Size([300])\n","Attention mask shape: torch.Size([300])\n","----------------------------------------------------------------------------------------------------\n"]}],"source":["for i in range(3):\n","    # Get original text\n","    original_text = train_dataset.reviews.iloc[i]\n","\n","    # Get preprocessed text by applying transform\n","    preprocessed_text = train_dataset.transform(original_text)\n","\n","    # Get tokenized tensor sample\n","    sample = train_dataset[i]\n","\n","    print(f\"\\nSample {i+1}:\")\n","    print(f\"Original text: {original_text[:200]}...\")\n","    print(f\"Preprocessed text: {preprocessed_text[:200]}...\")\n","    print(f\"Label: {sample['label']}\")\n","\n","    # Decode the tokens back to words to see what BERT sees\n","    tokens = train_dataset.tokenizer.convert_ids_to_tokens(sample['input_ids'])\n","    print(f\"First 30 tokens: {tokens[:30]}\")\n","    print(f\"Input tensor shape: {sample['input_ids'].shape}\")\n","    print(f\"Attention mask shape: {sample['attention_mask'].shape}\")\n","    print(\"-\" * 100)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"guBAcYVAeNHL","executionInfo":{"status":"ok","timestamp":1731221316702,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"}},"outputId":"b773fc82-e746-455e-857a-c244dc0807bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Attention mask (first 50 positions):\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0])\n","\n","Input IDs (first 50 positions):\n","tensor([  101,  2987,  2102,  2147,  2123,  2102,  4965,  2404,  2357,  2187,\n","         2234,  2188,  3147,  8962,  9868,  3538,  8013,  2326,  2507,  3042,\n","         6415,  2448,  2105,  2781,  2409,  2498,  2071,  3465,  6240,  1057,\n","         4965,  2028,  2204,  6735,  2123,  2102,  8572, 10231,  7685,  1039,\n","          102,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n","\n","First 10 tokens:\n","ID:   101 -> Token: [CLS]\n","ID:  2987 -> Token: doesn\n","ID:  2102 -> Token: ##t\n","ID:  2147 -> Token: work\n","ID:  2123 -> Token: don\n","ID:  2102 -> Token: ##t\n","ID:  4965 -> Token: buy\n","ID:  2404 -> Token: put\n","ID:  2357 -> Token: turned\n","ID:  2187 -> Token: left\n"]}],"source":["# Get a single sample\n","sample = train_dataset[0]\n","\n","# Look at where the attention mask is active (1) vs padding (0)\n","print(\"Attention mask (first 50 positions):\")\n","print(sample['attention_mask'][:50])\n","\n","# Look at the actual token IDs\n","print(\"\\nInput IDs (first 50 positions):\")\n","print(sample['input_ids'][:50])\n","\n","# Decode a few tokens\n","token_ids = sample['input_ids'][:10]\n","tokens = train_dataset.tokenizer.convert_ids_to_tokens(token_ids)\n","print(\"\\nFirst 10 tokens:\")\n","for id, token in zip(token_ids, tokens):\n","    print(f\"ID: {id:5d} -> Token: {token}\")"]},{"cell_type":"markdown","metadata":{"id":"MjaWK6hIgHrC"},"source":["## Load dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YBAkGWvreNHL","executionInfo":{"status":"ok","timestamp":1731221317122,"user_tz":-480,"elapsed":421,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"}},"outputId":"4d2798a8-a47d-4360-c240-a7930f631970"},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch input shape: torch.Size([8, 300])\n","Batch label shape: torch.Size([8])\n"]}],"source":["# Create DataLoader\n","from torch.utils.data import DataLoader\n","\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=8,\n","    shuffle=True,\n","    num_workers=2\n",")\n","\n","val_loader = DataLoader(\n","    val_dataset,\n","    batch_size=16,\n","    shuffle=False,\n","    num_workers=2\n",")\n","\n","# Test the dataloader\n","for batch in train_loader:\n","    print(f\"Batch input shape: {batch['input_ids'].shape}\")\n","    print(f\"Batch label shape: {batch['label'].shape}\")\n","    break"]},{"cell_type":"markdown","metadata":{"id":"IkXM0l1WgHrD"},"source":["# 3. Model"]},{"cell_type":"markdown","metadata":{"id":"6GWT8pfo4LRv"},"source":["## Transfer learning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kzY0w64eNHL","executionInfo":{"status":"ok","timestamp":1731221317509,"user_tz":-480,"elapsed":390,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"}},"outputId":"762c1a8b-d047-4fd8-ceab-3796626801f7"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","# Load pre-trained BERT model and configure for binary classification\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    'bert-base-uncased',\n","    num_labels=2,\n","    gradient_checkpointing=True  # Memory optimization\n",")\n","\n","# Move model to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n"]},{"cell_type":"markdown","metadata":{"id":"B194pqXMgHrD"},"source":["# 4. Training Setup"]},{"cell_type":"markdown","metadata":{"id":"IUv6IMQs21Pv"},"source":["## Hyperparameters for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g9nWjAVt2y5w"},"outputs":[],"source":["LEARNING_RATE = 0.001\n","MIN_LEARNING_RATE = 1e-6\n","NUM_EPOCHS = 100"]},{"cell_type":"markdown","metadata":{"id":"Ehx1FuZy2-LZ"},"source":["## Loss function, optimizer and learning rate scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Upy0mnVbgHrD"},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","# Define loss function and optimizer\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# Define learning rate scheduler\n","# Linear warmup + decay (recommended for BERT)\n","from transformers import get_linear_schedule_with_warmup\n","\n","num_training_steps = len(train_loader) * NUM_EPOCHS\n","num_warmup_steps = num_training_steps // 10  # 10% of total steps\n","\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=num_warmup_steps,\n","    num_training_steps=num_training_steps\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"gb2-g7_f3Lsv"},"source":["## Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4YcV4xg3Fws"},"outputs":[],"source":["# Define accuracy metrics for classification\n","def compute_correct(scores, targets):\n","    predictions = scores.max(axis=1)[1]\n","    correct = predictions.eq(targets).sum().item()\n","\n","    return correct"]},{"cell_type":"markdown","metadata":{"id":"mk8cWI4IgHrD"},"source":["# 5. Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":20423780,"status":"error","timestamp":1731241741285,"user":{"displayName":"Jane Bai","userId":"08410120511103737256"},"user_tz":-480},"id":"GLaq0gW7gHrD","outputId":"2cbdb756-c205-4f8b-b081-b270ec7d5131"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--------------------\n","Epoch [1/100]\n","Val Loss: 0.699817, Val Accuracy: 0.4965\n","Patience: [1/10]\n","\n","--------------------\n","Epoch [2/100]\n","Val Loss: 0.673638, Val Accuracy: 0.6305\n","\n","--------------------\n","Epoch [3/100]\n","Val Loss: 0.611086, Val Accuracy: 0.7155\n","\n","--------------------\n","Epoch [4/100]\n","Val Loss: 0.453030, Val Accuracy: 0.8390\n","\n","--------------------\n","Epoch [5/100]\n","Val Loss: 0.334264, Val Accuracy: 0.8830\n","\n","--------------------\n","Epoch [6/100]\n","Val Loss: 0.295683, Val Accuracy: 0.8805\n","Patience: [1/10]\n","\n","--------------------\n","Epoch [7/100]\n","Val Loss: 0.277447, Val Accuracy: 0.8855\n","\n","--------------------\n","Epoch [8/100]\n","Val Loss: 0.266331, Val Accuracy: 0.8925\n","\n","--------------------\n","Epoch [9/100]\n","Val Loss: 0.260924, Val Accuracy: 0.8945\n","\n","--------------------\n","Epoch [10/100]\n","Val Loss: 0.256662, Val Accuracy: 0.8975\n","\n","--------------------\n","Epoch [11/100]\n","Val Loss: 0.249780, Val Accuracy: 0.8995\n","\n","--------------------\n","Epoch [12/100]\n","Val Loss: 0.258981, Val Accuracy: 0.9005\n","\n","--------------------\n","Epoch [13/100]\n","Val Loss: 0.249431, Val Accuracy: 0.9060\n","\n","--------------------\n","Epoch [14/100]\n","Val Loss: 0.258378, Val Accuracy: 0.9090\n","\n","--------------------\n","Epoch [15/100]\n","Val Loss: 0.258212, Val Accuracy: 0.9080\n","Patience: [1/10]\n","\n","--------------------\n","Epoch [16/100]\n","Val Loss: 0.281917, Val Accuracy: 0.9000\n","Patience: [2/10]\n","\n","--------------------\n","Epoch [17/100]\n","Val Loss: 0.286446, Val Accuracy: 0.9050\n","Patience: [3/10]\n","\n","--------------------\n","Epoch [18/100]\n","Val Loss: 0.298614, Val Accuracy: 0.9065\n","Patience: [4/10]\n","\n","--------------------\n","Epoch [19/100]\n","Val Loss: 0.319816, Val Accuracy: 0.8990\n","Patience: [5/10]\n","\n","--------------------\n","Epoch [20/100]\n","Val Loss: 0.324950, Val Accuracy: 0.9070\n","Patience: [6/10]\n","\n","--------------------\n","Epoch [21/100]\n","Val Loss: 0.354780, Val Accuracy: 0.8980\n","Patience: [7/10]\n","\n","--------------------\n","Epoch [22/100]\n","Val Loss: 0.330706, Val Accuracy: 0.9065\n","Patience: [8/10]\n","\n","--------------------\n","Epoch [23/100]\n","Val Loss: 0.348205, Val Accuracy: 0.9100\n","\n","--------------------\n","Epoch [24/100]\n","Val Loss: 0.374465, Val Accuracy: 0.9085\n","Patience: [1/10]\n","\n","--------------------\n","Epoch [25/100]\n","Val Loss: 0.398187, Val Accuracy: 0.9080\n","Patience: [2/10]\n","\n","--------------------\n","Epoch [26/100]\n","Val Loss: 0.376747, Val Accuracy: 0.9020\n","Patience: [3/10]\n","\n","--------------------\n","Epoch [27/100]\n","Val Loss: 0.430003, Val Accuracy: 0.9020\n","Patience: [4/10]\n","\n","--------------------\n","Epoch [28/100]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-2f62e6c6710e>\u001b[0m in \u001b[0;36m<cell line: 155>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;31m# Run the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-24-2f62e6c6710e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, loss_function, optimizer, scheduler, num_epochs, patience)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-2f62e6c6710e>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, loss_function, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Move data to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Define training function for one epoch\n","def train_one_epoch(model, train_loader, loss_function, optimizer):\n","    model.train()\n","    total_loss = 0\n","    num_correct = 0\n","    num_total_inputs = 0\n","\n","    for batch in train_loader:\n","        # Move data to device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        targets = batch['label'].to(device)\n","\n","        # Forward pass\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits  # Get the logits from the output\n","        loss = loss_function(logits, targets)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Calculate accuracy\n","        # num_correct += compute_correct(logits, targets)\n","        # num_total_inputs += targets.size(0)\n","        # total_loss += loss.item()\n","\n","    # avg_loss = total_loss / len(train_loader)\n","    # accuracy = num_correct / num_total_inputs\n","    # return avg_loss, accuracy\n","\n","\n","# Define validation function for one epoch\n","def validate_one_epoch(model, val_loader, loss_function):\n","    model.eval()\n","    total_loss = 0\n","    num_correct = 0\n","    num_total_inputs = 0\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            # Move data to device\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            targets = batch['label'].to(device)\n","\n","            # Forward pass\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits  # Get the logits from the output\n","            loss = loss_function(logits, targets)\n","\n","            # Calculate accuracy\n","            num_correct += compute_correct(logits, targets)\n","            num_total_inputs += targets.size(0)\n","            total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(val_loader)\n","    accuracy = num_correct / num_total_inputs\n","    return avg_loss, accuracy\n","\n","# Define main training function\n","def train_model(model, train_loader, val_loader, loss_function, optimizer, scheduler, num_epochs, patience=5):\n","    # Log the loss and accuracy of each epoch\n","    training_history = {\n","        'train_loss': [], 'train_acc': [],\n","        'val_loss': [], 'val_acc': []\n","    }\n","    # Early stopping\n","    # Stop when the val_loss isn't improving after certain number of epochs (patience) and revert to the best model\n","    # This saves training time and prevents overfitting\n","    best_val_acc = 0.5\n","    best_model_state = None\n","    best_epoch = 0\n","    patience_counter = 0\n","\n","    for epoch in range(num_epochs):\n","        print(f'\\n{\"-\" * 20}\\nEpoch [{epoch+1}/{num_epochs}]')\n","\n","        # Training\n","        train_one_epoch(model, train_loader, loss_function, optimizer)\n","\n","        # Validation\n","        val_loss, val_acc = validate_one_epoch(model, val_loader, loss_function)\n","\n","        # Update learning rate, one time in each epoch\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","        # Update loss and acc history\n","        # training_history['train_loss'].append(train_loss)\n","        # training_history['train_acc'].append(train_acc)\n","        training_history['val_loss'].append(val_loss)\n","        training_history['val_acc'].append(val_acc)\n","\n","        # Print epoch summary\n","        print(\n","            # f'Train Loss: {train_loss:.6f}, Train Accuracy: {train_acc:.4f}\\n'\n","            f'Val Loss: {val_loss:.6f}, Val Accuracy: {val_acc:.4f}'\n","        )\n","\n","        # Early stopping logic\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            best_val_loss = val_loss\n","            best_model_state = model.state_dict()  # A dictionary containing all the model's parameters\n","            best_epoch = epoch\n","            patience_counter = 0  # Reset patience counter to zero\n","            # Save best model checkpoint\n","            # checkpoint = {\n","            #     'epoch': epoch,\n","            #     'model_state_dict': model.state_dict(),\n","            #     'optimizer_state_dict': optimizer.state_dict(),\n","            #     'scheduler_state_dict': scheduler.state_dict(),\n","\n","            #     'best_val_acc': best_val_acc,\n","            #     'best_val_loss': best_val_loss,\n","            #     'patience_counter': patience_counter,\n","\n","            #     'history': training_history,  # Save full history of loss and acc in each epoch\n","            # }\n","            # torch.save(checkpoint, 'best_model_checkpoint.pth')\n","            # print('Saved best model checkpoint.')\n","        else:\n","            patience_counter += 1\n","            print(f'Patience: [{patience_counter}/{patience}]')\n","\n","        # Save checkpoint\n","        # if epoch % 20 == 0:\n","        #     checkpoint = {\n","        #         'epoch': epoch,\n","        #         'model_state_dict': model.state_dict(),\n","        #         'optimizer_state_dict': optimizer.state_dict(),\n","        #         'scheduler_state_dict': scheduler.state_dict(),\n","\n","        #         'best_val_loss': best_val_loss,\n","        #         'patience_counter': patience_counter,\n","\n","        #         'history': training_history,  # Save full history of loss and acc in each epoch\n","        #     }\n","        #     torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pth')\n","        #     print('Saved model checkpoint.')\n","\n","        # Early stopping check (break training loop if patience reached)\n","        if patience_counter >= patience or val_acc > 0.95:\n","            print(f'Early stopping triggered at Epoch [{epoch+1}/{num_epochs}].')\n","            break  # Stop training\n","\n","    # Revert to best model\n","    if best_model_state is not None:\n","        model.load_state_dict(best_model_state)\n","        print(f'Restored best model state. Epoch [{best_epoch+1}/{num_epochs}]')\n","\n","# Run the training loop\n","train_model(model, train_loader, val_loader, loss_function, optimizer, scheduler=scheduler, num_epochs=NUM_EPOCHS, patience=10)"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["qLf4TL23eNHI"],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}